{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git submodule update --recursive --remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta sync -mbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta analyse rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta analyse upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pasta prepare_evaluation --review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!./pasta prepare_evaluation --ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pypasta.Config import Config\n",
    "\n",
    "config = Config('linux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config.f_responses_pkl, 'rb') as handle:\n",
    "    response_df = pd.DataFrame(pickle.load(handle))\n",
    "response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df['upstream'] = response_df['upstream'].map(list)\n",
    "\n",
    "response_df.index.name = \"idx\"\n",
    "\n",
    "response_df.fillna({'patch_id': '_'}, inplace=True)\n",
    "print(\"Filled NA for patch_id\")\n",
    "\n",
    "response_df.set_index(['patch_id'], append=True, inplace=True)\n",
    "print(\"Done setting index for response_df\")\n",
    "\n",
    "# Denormalize\n",
    "df_melt_responses = pd.melt(response_df.responses.apply(pd.Series).reset_index(),\n",
    "                            id_vars=['idx', 'patch_id'],\n",
    "                            value_name='responses').sort_index()\n",
    "\n",
    "df_melt_responses.drop('variable', axis=1, inplace=True)\n",
    "\n",
    "print(\"melt_responses_shape {}\".format(df_melt_responses.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flat_table\n",
    "\n",
    "df_denorm_responses = flat_table.normalize(df_melt_responses, expand_dicts=True, expand_lists=True)\n",
    "df_denorm_responses.drop('index', axis=1, inplace=True)\n",
    "df_denorm_responses.drop_duplicates(inplace=True)\n",
    "print(\"Computed de-normalized responses, writing to disk...\")\n",
    "df_denorm_responses.to_csv('resources/linux/resources/df_denorm_responses.csv', index=False)\n",
    "print(\"Processed responses!\")\n",
    "\n",
    "df_melt_upstream = pd.melt(response_df.upstream.apply(pd.Series).reset_index(),\n",
    "                           id_vars=['idx', 'patch_id'],\n",
    "                           value_name='upstream').sort_index()\n",
    "\n",
    "df_melt_upstream.drop('variable', axis=1, inplace=True)\n",
    "df_melt_upstream.drop_duplicates(inplace=True)\n",
    "\n",
    "df_melt_upstream.to_csv('resources/linux/resources/df_denorm_upstream.csv', index=False)\n",
    "print(\"Processed upstream!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "def try_literal_eval(s):\n",
    "    try:\n",
    "        return literal_eval(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "def _get_message_field(msg, field):\n",
    "    if not (np.all(pd.isnull(msg))):\n",
    "        return email.message_from_bytes(msg)[field]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "dd1 = df_denorm_responses.set_index(['idx'])\n",
    "\n",
    "dd2 = df_melt_upstream.set_index(['idx'])\n",
    "\n",
    "df_dask_final = dd.merge(dd1, dd2, left_index=True, right_index=True, how='left') \\\n",
    "    .drop(['patch_id_y'], axis=1) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .rename(columns={\"patch_id_x\": \"patch_id\"})\n",
    "\n",
    "df_dask_final.to_csv(\"resources/linux/resources/df_dask_final.csv\")\n",
    "\n",
    "final = dd.read_csv(\"resources/linux/resources/df_dask_final.csv\", blocksize=50e7, \n",
    "                    dtype={\"idx \": \"int32\", \"patch_id \": \"category\",\n",
    "                           \"responses.resp_msg_id\": \"category\",\n",
    "                           \"responses.parent\": \"category\",\n",
    "                           \"upstream\": \"category\"}).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print(\"Final shape with possible duplicate rows{}\".format(final.shape))\n",
    "final.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert to pandas\n",
    "df_pd_final = final.compute()\n",
    "\n",
    "# Remove rows with no patch and other infos\n",
    "index_names = df_pd_final[(df_pd_final['patch_id'] == '_') &\n",
    "                          (df_pd_final['upstream'].isna())].index\n",
    "df_pd_final.drop(index_names, inplace=True)\n",
    "\n",
    "print(\"Final shape after removing duplicates {}\".format(final.shape))\n",
    "\n",
    "# df_pd_final.to_csv(config.f_merged_responses_upstream, index=False)\n",
    "# print(\"Finished writing de-duplicated pandas merged dataframe to disk\")\n",
    "\n",
    "final = dd.from_pandas(df_pd_final, npartitions=20)\n",
    "\n",
    "final.reset_index().rename(columns={'index': 'idx'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('resources/linux/resources/final.csv', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pd_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_denorm_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_melt_upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_dask_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "final = dd.read_csv('resources/linux/resources/final.csv', blocksize=50e7,\n",
    "                        dtype={\"idx \": \"int32\",\n",
    "                               \"patch_id \": \"category\",\n",
    "                               \"responses.parent\": \"category\",\n",
    "                               \"upstream\": \"category\",\n",
    "                               \"response_author\": \"category\"}).drop('Unnamed: 0', axis=1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypasta import Config\n",
    "from pypasta.Repository.Mbox import Mbox\n",
    "\n",
    "config = Config('linux')\n",
    "\n",
    "\n",
    "repo = config.repo\n",
    "repo.register_mbox(config)\n",
    "repo.mbox.load_threads()\n",
    "\n",
    "# Discard null patches (coming from upstreams that were not mapped to any patch emails)\n",
    "unique_patches = set(final.patch_id.unique().compute())\n",
    "unique_patches.discard('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import email\n",
    "import chardet\n",
    "\n",
    "def get_relevant_patches(characteristics):\n",
    "    # First, we have to define the term 'relevant patch'. For our analysis, we\n",
    "    # must only consider patches that either fulfil rule 1 or 2:\n",
    "    #\n",
    "    # 1. Patch is the parent of a thread.\n",
    "    #    This covers classic one-email patches\n",
    "    #\n",
    "    # 2. Patch is the 1st level child of the parent of a thread\n",
    "    #    In this case, the parent can either be a patch (e.g., a series w/o\n",
    "    #    cover letter) or not a patch (e.g., parent is a cover letter)\n",
    "    #\n",
    "    # 3. The patch must not be sent from a bot (e.g., tip-bot)\n",
    "    #\n",
    "    # 4. Ignore stable review patches\n",
    "    #\n",
    "    # All other patches MUST be ignored. Rationale: Maintainers may re-send\n",
    "    # the patch as a reply of the discussion. Such patches must be ignored.\n",
    "    # Example: Look at the thread of\n",
    "    #     <20190408072929.952A1441D3B@finisterre.ee.mobilebroadband>\n",
    "    #\n",
    "    # Furthermore, only consider patches that actually patch Linux (~14% of all\n",
    "    # patches on Linux MLs patch other projects). Then only consider patches\n",
    "    # that are not for next, not from bots (there are a lot of bots) and that\n",
    "    # are no 'process mails' (e.g., pull requests)\n",
    "\n",
    "    relevant = set()\n",
    "\n",
    "    all_messages = 0\n",
    "    skipped_bot = 0\n",
    "    skipped_stable = 0\n",
    "    skipped_not_linux = 0\n",
    "    skipped_no_patch = 0\n",
    "    skipped_not_first_patch = 0\n",
    "    skipped_process = 0\n",
    "    skipped_next = 0\n",
    "\n",
    "    for m, c in characteristics.items():\n",
    "        skip = False\n",
    "        all_messages += 1\n",
    "\n",
    "        if not c.is_patch:\n",
    "            skipped_no_patch += 1\n",
    "            continue\n",
    "\n",
    "        if not c.patches_linux:\n",
    "            skipped_not_linux += 1\n",
    "            skip = True\n",
    "        if not c.is_first_patch_in_thread:\n",
    "            skipped_not_first_patch += 1\n",
    "            skip = True\n",
    "\n",
    "        if c.is_from_bot:\n",
    "            skipped_bot += 1\n",
    "            skip = True\n",
    "        if c.is_stable_review:\n",
    "            skipped_stable += 1\n",
    "            skip = True\n",
    "        if c.process_mail:\n",
    "            skipped_process += 1\n",
    "            skip = True\n",
    "        if c.is_next:\n",
    "            skipped_next += 1\n",
    "            skip = True\n",
    "\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        relevant.add(m)\n",
    "\n",
    "    print('')\n",
    "    print('=== Calculation of relevant patches ===')\n",
    "    print('All messages: %u' % all_messages)\n",
    "    print('  No patches: %u' % skipped_no_patch)\n",
    "    print('Skipped patches:')\n",
    "    print('  Not Linux: %u' % skipped_not_linux)\n",
    "    print('  Bot: %u' % skipped_bot)\n",
    "    print('  Stable: %u' % skipped_stable)\n",
    "    print('  Process mail: %u' % skipped_process)\n",
    "    print('  Next: %u' % skipped_next)\n",
    "    print('Relevant patches: %u' % len(relevant))\n",
    "\n",
    "    return relevant\n",
    "\n",
    "def _is_response_from_bot(message):\n",
    "    lmc = LinuxMailCharacteristics(repo, None, None, message)\n",
    "    flag = lmc.is_from_bot\n",
    "    return message, flag\n",
    "\n",
    "def check_person_duplicates(patch_id, resp_msg_id, author1, author2):\n",
    "    try:\n",
    "        name1, email1 = author1\n",
    "        name2, email2 = author2\n",
    "        if email1 == email2:\n",
    "            return True\n",
    "        if name1 == name2:\n",
    "            return True\n",
    "        return fuzz.token_sort_ratio(name1, name2) >= 80\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error parsing authors for patch id {} and response {}: author1 {} and author2 {}\"\n",
    "                 .format(patch_id, resp_msg_id, author1, author2))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypasta.LinuxMailCharacteristics import load_linux_mail_characteristics, email_get_from, LinuxMailCharacteristics\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "_, clustering = config.load_cluster()\n",
    "clustering.optimize()\n",
    "\n",
    "patch_characteristics = load_linux_mail_characteristics(config, None, clustering, unique_patches)\n",
    "\n",
    "# Consider only relevant patches (as per given definition of relevance)\n",
    "relevant_patches = get_relevant_patches(patch_characteristics)\n",
    "final_filtered_1 = final[final['patch_id'].isin(relevant_patches)]\n",
    "\n",
    "# Filter responses -- only responses to the patch itself count as a response, and not the rest of the thread emails\n",
    "final_filtered_2 = final_filtered_1[final_filtered_1['patch_id'] == final_filtered_1['responses.parent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = Pool(processes=int(cpu_count()), maxtasksperchild=1)\n",
    "response_to_bot = p1.map(_is_response_from_bot, list(final_filtered_2['responses.resp_msg_id'].unique().compute()),\n",
    "                         chunksize=1000)\n",
    "p1.close()\n",
    "p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "response_bot_df = pd.DataFrame(response_to_bot, columns=['responses.resp_msg_id', 'response_is_bot'])\n",
    "\n",
    "final_filtered_2 = dd.merge(final_filtered_2, response_bot_df, how='left', on=['responses.resp_msg_id'])\n",
    "\n",
    "if 'response_is_bot_x' in final_filtered_2.columns:\n",
    "    final_filtered_2 = final_filtered_2.drop(['response_is_bot_x'], axis=1) \\\n",
    "        .rename(columns={\"response_is_bot_y\": \"response_is_bot\"})\n",
    "\n",
    "# Remove duplicate rows with response message id, upstream, and patch_id (artifact of denormalization?)\n",
    "final_dedup = final_filtered_2.drop_duplicates(subset=['responses.resp_msg_id', 'upstream', 'patch_id'],\n",
    "                                               keep='first')\n",
    "\n",
    "# Rename some columns, removing the 'responses.' prefix to simplify dataframe Series ops\n",
    "new_columns = ['patch_id', 'response_author', 'resp_parent', 'resp_msg_id', 'upstream', 'response_is_bot']\n",
    "final_dedup = final_dedup.rename(columns=dict(zip(final_dedup.columns, new_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parseaddr_unicode(addr) -> (str, str):\n",
    "    \"\"\"Like parseaddr but return name in unicode instead of in RFC 2047 format\n",
    "    '=?UTF-8?B?TmjGoW4gTmd1eeG7hW4=?= <abcd@gmail.com>' -> ('Nhơn Nguyễn', \"abcd@gmail.com\")\n",
    "    \"\"\"\n",
    "    # name, e_mail = email.utils.parseaddr(addr)\n",
    "    # e_mail = e_mail.strip().lower()\n",
    "    name, e_mail = addr\n",
    "    name_list = []\n",
    "    if name:\n",
    "        name = name.strip()\n",
    "\n",
    "        for decoded_string, charset in email.header.decode_header(name):\n",
    "            if charset is not None:\n",
    "\n",
    "                try:\n",
    "                    if isinstance(decoded_string, bytes):\n",
    "                        name = decoded_string.decode(charset or 'utf-8')\n",
    "                    else:\n",
    "                        name = str(decoded_string, 'utf-8', errors='ignore')\n",
    "                except UnicodeDecodeError:\n",
    "                    encoding = chardet.detect(decoded_string)['encoding']\n",
    "                    try:\n",
    "                        name = decoded_string.decode(encoding)\n",
    "                    except TypeError:\n",
    "                        name = str(decoded_string, 'utf-8', errors='ignore')\n",
    "            else:\n",
    "                name = str(decoded_string)\n",
    "            name_list.append(name)\n",
    "\n",
    "    final_name = u''.join(name_list)\n",
    "    return final_name, e_mail\n",
    "\n",
    "def get_patch_author(message, repo):\n",
    "    try:\n",
    "        msg = repo.mbox.get_messages(message)[0]\n",
    "        return email_get_from(msg)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return email_get_from(message)\n",
    "\n",
    "final_dedup['patch_author'] = final_dedup['patch_id'].map(lambda x: get_patch_author(x, repo),\n",
    "                                                          meta=pd.Series([], dtype=object, name='x'))\n",
    "\n",
    "final_dedup['responder'] = final_dedup['resp_msg_id'].map(lambda x: get_patch_author(x, repo),\n",
    "                                                          meta=pd.Series([], dtype=object, name='x'))\n",
    "\n",
    "# This flag could detect authors responding themselves to the patches, e.g., responses to patches as rest\n",
    "# of the patch series (spotted often this case)\n",
    "final_dedup['self_response'] = final_dedup.map_partitions(lambda df: df.apply(\n",
    "    (lambda row: check_person_duplicates(row.patch_id, row.resp_msg_id, row.patch_author, row.responder)),\n",
    "    axis=1), meta=pd.Series([], dtype=object, name='row'))\n",
    "\n",
    "final_dedup.to_csv('resources/linux/resources/filtered_responses.csv', single_file=True)\n",
    "\n",
    "print(\"Written filtered response dataframe to disk, Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
